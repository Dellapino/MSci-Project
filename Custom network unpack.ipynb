{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store weighted undirected network as weighted array as follows\n",
    "\n",
    "{\n",
    "'node': [['edge', 'edge', 'edge'], ['weight', 'weight', 'weight']]\n",
    "}\n",
    "\n",
    "Then save this to an easily accessible file -> This will require me to save as a json file and require me to code another set of functions to interpret these graphs, I will simply use the current files one hand.\n",
    "\n",
    "Also, consider splitting weighting and connections into multiple dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "28820 492 938\n",
      "\n",
      "<class 'str'>\n",
      "28860 267 272\n",
      "\n",
      "<class 'str'>\n",
      "29300 181 826\n",
      "\n",
      "<class 'str'>\n",
      "29780 79 181\n",
      "\n",
      "<class 'str'>\n",
      "30000 150 196\n",
      "\n",
      "<class 'list'>\n",
      "['28820', '492', '938']\n",
      "['28860', '267', '272']\n",
      "['29300', '181', '826']\n",
      "['29780', '79', '181']\n",
      "['30000', '150', '196']\n",
      "9827\n"
     ]
    }
   ],
   "source": [
    "# Taking a look at the data\n",
    "\n",
    "data = open('Sociopatterns datasets/workplace_2013.dat', 'r')\n",
    "\n",
    "for i in range(5):\n",
    "    line = data.readline()\n",
    "    print(type(line))\n",
    "    print(line)\n",
    "    \n",
    "data = open('Sociopatterns datasets/workplace_2013.dat', 'r')\n",
    "\n",
    "lines = data.readlines()\n",
    "print(type(lines))\n",
    "\n",
    "for i in range(5):\n",
    "    print(lines[i].split())\n",
    "    \n",
    "print(len(lines))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Go through data by time (not needed)\n",
    "\n",
    "Look at all interactions occuring at a certain time\n",
    "\n",
    "Add weight to the interaction between those two nodes\n",
    "\n",
    "start populating a dictionary looking at the data\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('Sociopatterns datasets/workplace_2013.dat', 'r')\n",
    "\n",
    "lines = data.readlines() \n",
    "lines = [line.split() for line in lines] # 2D array containing lines from data, split into words stored as strings\n",
    "length = len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalised\n",
    "\n",
    "def access_dataset(name):\n",
    "    data = open(name, 'r')\n",
    "    lines = data.readlines() \n",
    "    lines = [line.split() for line in lines] # 2D array containing lines from data, split into words stored as strings\n",
    "    return lines\n",
    "\n",
    "def build_dict_1(network, lines):\n",
    "    for i in range(len(lines)): # This works! but generates a directed network\n",
    "        a = lines[i][1] # first person in interaction\n",
    "        b = lines[i][2] # second person in interaction\n",
    "        \n",
    "        try: # editing an existing node\n",
    "            if b in network[a][0]:\n",
    "                index = network[a][0].index(b)\n",
    "                network[a][1][index] += 1 # increasing interaction strength\n",
    "            else:\n",
    "                network[a][0].append(b)\n",
    "                network[a][1].append(1)\n",
    "        except: # adding new interaction\n",
    "            network[a] = [[b], [1]]\n",
    "    \n",
    "    return network\n",
    "\n",
    "def build_dict_2(network, lines):\n",
    "    for i in range(len(lines)): # This works! but generates a directed network\n",
    "        b = lines[i][1] # first person in interaction\n",
    "        a = lines[i][2] # second person in interaction\n",
    "        \n",
    "        try: # editing an existing node\n",
    "            if b in network[a][0]:\n",
    "                index = network[a][0].index(b)\n",
    "                network[a][1][index] += 1 # increasing interaction strength\n",
    "            else:\n",
    "                network[a][0].append(b)\n",
    "                network[a][1].append(1)\n",
    "        except: # adding new interaction\n",
    "            network[a] = [[b], [1]]\n",
    "    \n",
    "    return network \n",
    "    \n",
    "def build_network(name):\n",
    "    lines = access_dataset(name)\n",
    "    network = {}\n",
    "    network = build_dict_1(network, lines)\n",
    "    network = build_dict_2(network, lines)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 242\n",
      "Number of unique nodes: 242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# sanity check to ensure that correct number of people exist on the graph\n",
    "def check_size(network):\n",
    "    vals = network.keys()\n",
    "    test_unique = []\n",
    "    for i in vals:\n",
    "        if i in test_unique:\n",
    "            pass\n",
    "        else:\n",
    "            test_unique.append(i)\n",
    "    nodes = len(test_unique)\n",
    "    print('Number of nodes: ' + str(len(vals)))\n",
    "    print('Number of unique nodes: ' + str(nodes))\n",
    "    return nodes\n",
    "    \n",
    "\n",
    "check_size(primaryschool_network_1)\n",
    "\n",
    "\n",
    "# This turned out to be quite redundant ngl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "workplace_network_1 = build_network('Sociopatterns datasets/workplace_2013.dat')\n",
    "workplace_network_2 = build_network('Sociopatterns datasets/workplace_2015.dat_')\n",
    "highschool_network_1 = build_network('Sociopatterns datasets/highschool_2011.csv')\n",
    "highschool_network_2 = build_network('Sociopatterns datasets/highschool_2012.csv')\n",
    "highschool_network_3 = build_network('Sociopatterns datasets/highschool_2013.csv')\n",
    "primaryschool_network_1 = build_network('Sociopatterns datasets/primaryschool_2014.csv')\n",
    "hospital_network_1 = build_network('Sociopatterns datasets/hospital_2010.dat_')\n",
    "conference_network_1 = build_network('Sociopatterns datasets/conference_2009.dat_')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = [\n",
    "'Sociopatterns datasets/conference_2009.dat_',\n",
    "'Sociopatterns datasets/highschool_2011.csv',\n",
    "'Sociopatterns datasets/highschool_2012.csv',\n",
    "'Sociopatterns datasets/highschool_2013.csv',\n",
    "'Sociopatterns datasets/hospital_2010.dat_',\n",
    "'Sociopatterns datasets/primaryschool_2014.csv',\n",
    "'Sociopatterns datasets/workplace_2013.dat',\n",
    "'Sociopatterns datasets/workplace_2015.dat_']\n",
    "\n",
    "dataset_names = [\n",
    "    'Conference 1', \n",
    "    'Highschool 1', \n",
    "    'Highschool 2', \n",
    "    'Highschool 3', \n",
    "    'Hospital 1', \n",
    "    'Primaryschool 1', \n",
    "    'Workplace 1', \n",
    "    'Workplace 2']\n",
    "\n",
    "networks = []\n",
    "\n",
    "for i in range(len(dataset_paths)):\n",
    "    networks.append(build_network(dataset_paths[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_connnections(network):\n",
    "    nodes = list(network.keys())\n",
    "    connections = []\n",
    "    for i in range(len(network)):\n",
    "        connections.append(network[nodes[i]][0])\n",
    "    return connections\n",
    "\n",
    "def find_strengths(network):\n",
    "    nodes = list(network.keys())\n",
    "    strengths = []\n",
    "    for i in range(len(network)):\n",
    "        strengths.append(network[nodes[i]][1])\n",
    "    \n",
    "    norm = max(max(s) for s in strengths)\n",
    "    \n",
    "    for i in range(len(network)):\n",
    "        for j in range(len(strengths[i])):\n",
    "            strengths[i][j] = strengths[i][j] / norm\n",
    "    \n",
    "    return strengths\n",
    "\n",
    "# check that strengths are correctly normalised:\n",
    "\n",
    "stronk = find_strengths(networks[0])\n",
    "check = max(max(s) for s in stronk)\n",
    "print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is built from interactions over 11.5 days\n",
      "nice\n"
     ]
    }
   ],
   "source": [
    "def find_time(path):\n",
    "    data = access_dataset(path)\n",
    "    start = data[0][0]\n",
    "    end = data[-1][0]\n",
    "    time = round((1022380 - 28840) / 60 / 60 / 24, 1)\n",
    "    print('The dataset is built from interactions over ' + str(time) + ' days')\n",
    "    return time\n",
    "\n",
    "find_time('Sociopatterns datasets/conference_2009.dat_')\n",
    "print('nice')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25e1ce75cfb12f047913573b7bc7a98ece3d58460e313b6b0bb12faf016c0990"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
